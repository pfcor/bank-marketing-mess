{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BANK MARKETING\n",
    "\n",
    "<br><br>\n",
    "Membros:\n",
    "- Anderson Jesus\n",
    "- Caio Viera\n",
    "- Pedro Correia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CRIAÇÃO DE MODELO PREDITIVO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando sessão do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/pfcor/spark-2.1.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('bank').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = spark.read.csv(\n",
    "#     \"hdfs://elephant:8020/user/labdata/bank.csv\",\n",
    "#     header=True,\n",
    "#     sep=\";\",\n",
    "#     inferSchema=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    'data/historical-data.csv',\n",
    "    sep=';',\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.selectExpr(*[\"`{}` as {}\".format(col, col.replace('.', '_')) for col in data.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo variáveis utilizadas pelo tipo a fim de realizar o encoding necessário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'month',\n",
    "    'day_of_week',\n",
    "    'poutcome'\n",
    "]\n",
    "\n",
    "# não utilizaremos a variável `duration`, que algo não sabido antes da ligação ocorrer\n",
    "# e portanto, não deve ser válida para fins preditivos\n",
    "numericColumns = [\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'emp_var_rate',\n",
    "    'cons_price_idx',\n",
    "    'cons_conf_idx',\n",
    "    'euribor3m',\n",
    "    'nr_employed'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando a construção do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders e indexadores necessários ao tratamento dos dados\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "# instanciando nossa lista de passos a serem fornecidos ao pipeline\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexação da variável resposta\n",
    "indexer = StringIndexer(\n",
    "    inputCol='y', \n",
    "    outputCol='label'\n",
    ")\n",
    "\n",
    "stages += [indexer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados Categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformações dados categóricos (paralelo pandas: get_dummies)\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # nomes para valores [0:n_cats-1]\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=categoricalCol, \n",
    "        outputCol=categoricalCol+'_index'\n",
    "    )\n",
    "    # criando dummies\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=categoricalCol+'_index',\n",
    "        outputCol=categoricalCol+'_class_vec'\n",
    "    )\n",
    "    # inserindo estágios de transformação\n",
    "    stages += [indexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dados Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando variáveis numéricas para o tipo double\n",
    "for numericCol in numericColumns:\n",
    "    data = data.withColumn(numericCol, data[numericCol].cast('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando assembler, que deixa os dados no formato vetorial \n",
    "# demandado pela biblioteca ML do Spark\n",
    "\n",
    "assembler_inputs = [categoricalCol+'_class_vec' for categoricalCol in categoricalColumns]\n",
    "assembler_inputs += numericColumns\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação do Pipeline finalizada e fit/transform realizado na base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciando Pipeline com os estágios criados acima\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# fit na base\n",
    "pipelineModel = pipeline.fit(data)\n",
    "\n",
    "# salvando o pipeline\n",
    "pipelineModel.write().overwrite().save('model/bank-pipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando enfim os dados para o treinar o modelo\n",
    "data_model = pipelineModel.transform(data)\n",
    "data_model = data_model.select([\"label\", \"features\"]) # reduzindo a quantidade de dados a serem processados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Boosting Machine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando bases de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações para treino: 29705\n",
      "Observações para teste:  7364\n"
     ]
    }
   ],
   "source": [
    "# separação de dados em treino e teste\n",
    "(trainingData, testData) = data_model.randomSplit([0.8, 0.2], seed=420)\n",
    "\n",
    "print('Observações para treino: {}'.format(trainingData.count()))\n",
    "print('Observações para teste:  {}'.format(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando modelagem propriamente dita (hiperparâmetros baseados em exploratória realizada anteriormente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    predictionCol='prediction',\n",
    "    maxIter=60,\n",
    "    stepSize=0.01,\n",
    "    seed=420\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 1.71 ms, total: 17.7 ms\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbtModel = gbt.fit(trainingData)\n",
    "\n",
    "# salvando o modelo\n",
    "gbtModel.write().overwrite().save('model/bank-gbtmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBTClassificationModel (uid=GBTClassifier_44f8833c837bf6bc4c35) with 60 trees\n"
     ]
    }
   ],
   "source": [
    "print(gbtModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt = gbtModel.transform(testData)\n",
    "predictions_gbt_train = gbtModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando qualidade das predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.9005\n",
      "Accuracy (TRAIN): 0.9054\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# evaluator_auc = BinaryClassificationEvaluator(\n",
    "#     labelCol=\"label\", \n",
    "#     rawPredictionCol=\"rawPrediction\"\n",
    "# )\n",
    "\n",
    "accuracy_gbt = evaluator_accuracy.evaluate(predictions_gbt)\n",
    "accuracy_gbt_train = evaluator_accuracy.evaluate(predictions_gbt_train)\n",
    "print('Accuracy:         {:.4f}'.format(accuracy_gbt))\n",
    "print('Accuracy (TRAIN): {:.4f}'.format(accuracy_gbt_train))\n",
    "# auc_gbt = evaluator_auc.evaluate(predictions_gbt)\n",
    "# print('areaUnderROC:     {:.4f}'.format(auc_gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.9005|   0.6369|0.2518|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt.select('label', 'prediction').createOrReplaceTempView('predictions')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusão\n",
    "\n",
    "`GBTClassifier` apresentou maior potencial, apesar de ter um recall ainda muito baixo. \n",
    "\n",
    "Na sequência, vamos buscar realizar um resampling da base, buscando deixar a classe positiva mais prevalente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_res, holdOutData) = data_model_res.randomSplit([0.8, 0.2], seed=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_res = data_res.sampleBy('y', fractions={'yes': 0.8, 'no': 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  y|count|\n",
      "+---+-----+\n",
      "| no| 1050|\n",
      "|yes| 2069|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_res.groupBy('y').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, a quantidade de dados utilizados diminuiu bastante. Assim, esperamos uma redução na acurácia do modelo, mas a espectativa é que o recall melhore, o que é o que nós buscamos nesse momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessamento\n",
    "# data_model_res = pipelineModel.transform(data_res)\n",
    "\n",
    "# (trainingData_res, testData_res) = data_model_res.randomSplit([0.8, 0.2], seed=420)\n",
    "# print('Observações para treino: {}'.format(trainingData_res.count()))\n",
    "# print('Observações para teste:  {}'.format(testData_res.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando novo modelo com resampling\n",
    "gbtModel_res = gbt.fit(data_res)\n",
    "\n",
    "# salvando o modelo\n",
    "gbtModel.write().overwrite().save('model/bank-gbtmodel-res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt_res = gbtModel_res.transform(testData_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.7923\n"
     ]
    }
   ],
   "source": [
    "accuracy_gbt_res = evaluator_accuracy.evaluate(predictions_gbt_res)\n",
    "print('Accuracy:         {:.4f}'.format(accuracy_gbt_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.7923|   0.7661|0.5663|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt_res.select('label', 'prediction').createOrReplaceTempView('predictions_res')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions_res\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperávamos, o recall apresentou um valor bem mais interessante, demonstrando o potencial da solução, que certamente poderia ser refinada ainda mais a fim de atingir resultados mais expressivos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
