{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BANK MARKETING\n",
    "\n",
    "<br><br>\n",
    "Membros:\n",
    "- Anderson\n",
    "- Caio Viera\n",
    "- Pedro Correia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inicializando sessão do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo os dados do HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    \"hdfs://elephant:8020/user/labdata/bank.csv\",\n",
    "    header=True,\n",
    "    sep=\";\",\n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.selectExpr(*[\"`{}` as {}\".format(col, col.replace('.', '_')) for col in data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- emp_var_rate: double (nullable = true)\n",
      " |-- cons_price_idx: double (nullable = true)\n",
      " |-- cons_conf_idx: double (nullable = true)\n",
      " |-- euribor3m: double (nullable = true)\n",
      " |-- nr_employed: double (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job</th>\n",
       "      <td>housemaid</td>\n",
       "      <td>services</td>\n",
       "      <td>services</td>\n",
       "      <td>admin.</td>\n",
       "      <td>services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital</th>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "      <td>married</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>basic.4y</td>\n",
       "      <td>high.school</td>\n",
       "      <td>high.school</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>high.school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>default</th>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact</th>\n",
       "      <td>telephone</td>\n",
       "      <td>telephone</td>\n",
       "      <td>telephone</td>\n",
       "      <td>telephone</td>\n",
       "      <td>telephone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>may</td>\n",
       "      <td>may</td>\n",
       "      <td>may</td>\n",
       "      <td>may</td>\n",
       "      <td>may</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_of_week</th>\n",
       "      <td>mon</td>\n",
       "      <td>mon</td>\n",
       "      <td>mon</td>\n",
       "      <td>mon</td>\n",
       "      <td>mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration</th>\n",
       "      <td>261</td>\n",
       "      <td>149</td>\n",
       "      <td>226</td>\n",
       "      <td>151</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pdays</th>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poutcome</th>\n",
       "      <td>nonexistent</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>nonexistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp_var_rate</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons_price_idx</th>\n",
       "      <td>93.994</td>\n",
       "      <td>93.994</td>\n",
       "      <td>93.994</td>\n",
       "      <td>93.994</td>\n",
       "      <td>93.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <td>-36.4</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>-36.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>euribor3m</th>\n",
       "      <td>4.857</td>\n",
       "      <td>4.857</td>\n",
       "      <td>4.857</td>\n",
       "      <td>4.857</td>\n",
       "      <td>4.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nr_employed</th>\n",
       "      <td>5191</td>\n",
       "      <td>5191</td>\n",
       "      <td>5191</td>\n",
       "      <td>5191</td>\n",
       "      <td>5191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0            1            2            3  \\\n",
       "age                      56           57           37           40   \n",
       "job               housemaid     services     services       admin.   \n",
       "marital             married      married      married      married   \n",
       "education          basic.4y  high.school  high.school     basic.6y   \n",
       "default                  no      unknown           no           no   \n",
       "housing                  no           no          yes           no   \n",
       "loan                     no           no           no           no   \n",
       "contact           telephone    telephone    telephone    telephone   \n",
       "month                   may          may          may          may   \n",
       "day_of_week             mon          mon          mon          mon   \n",
       "duration                261          149          226          151   \n",
       "campaign                  1            1            1            1   \n",
       "pdays                   999          999          999          999   \n",
       "previous                  0            0            0            0   \n",
       "poutcome        nonexistent  nonexistent  nonexistent  nonexistent   \n",
       "emp_var_rate            1.1          1.1          1.1          1.1   \n",
       "cons_price_idx       93.994       93.994       93.994       93.994   \n",
       "cons_conf_idx         -36.4        -36.4        -36.4        -36.4   \n",
       "euribor3m             4.857        4.857        4.857        4.857   \n",
       "nr_employed            5191         5191         5191         5191   \n",
       "y                        no           no           no           no   \n",
       "\n",
       "                          4  \n",
       "age                      56  \n",
       "job                services  \n",
       "marital             married  \n",
       "education       high.school  \n",
       "default                  no  \n",
       "housing                  no  \n",
       "loan                    yes  \n",
       "contact           telephone  \n",
       "month                   may  \n",
       "day_of_week             mon  \n",
       "duration                307  \n",
       "campaign                  1  \n",
       "pdays                   999  \n",
       "previous                  0  \n",
       "poutcome        nonexistent  \n",
       "emp_var_rate            1.1  \n",
       "cons_price_idx       93.994  \n",
       "cons_conf_idx         -36.4  \n",
       "euribor3m             4.857  \n",
       "nr_employed            5191  \n",
       "y                        no  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toPandas().head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = [\n",
    "    'job',\n",
    "    'marital',\n",
    "    'education',\n",
    "    'default',\n",
    "    'housing',\n",
    "    'loan',\n",
    "    'contact',\n",
    "    'month',\n",
    "    'day_of_week',\n",
    "    'poutcome'\n",
    "]\n",
    "\n",
    "numericColumns = [\n",
    "    'pdays',\n",
    "    'previous',\n",
    "    'emp_var_rate',\n",
    "    'cons_price_idx',\n",
    "    'cons_conf_idx',\n",
    "    'euribor3m',\n",
    "    'nr_employed'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Categories:\n",
      "  - job          12\n",
      "  - marital      4\n",
      "  - education    8\n",
      "  - default      3\n",
      "  - housing      3\n",
      "  - loan         3\n",
      "  - contact      2\n",
      "  - month        10\n",
      "  - day_of_week  5\n",
      "  - poutcome     3\n"
     ]
    }
   ],
   "source": [
    "# checando cardinalidade das colunas categóricas\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "print('Distinct Categories:')\n",
    "for categoricalCol in categoricalColumns:\n",
    "    print('  - {:<12} {}'.format(categoricalCol, data.agg(countDistinct(categoricalCol)).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nenhuma das variáveis categóricas apresenta um caso grave de cardinalidade excessiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformações dados categóricos\n",
    "for categoricalCol in categoricalColumns:\n",
    "    # nomes para valores [0:n_cats-1]\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=categoricalCol, \n",
    "        outputCol=categoricalCol+'_index'\n",
    "    )\n",
    "    # criando dummies\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=categoricalCol+'_index',\n",
    "        outputCol=categoricalCol+'_class_vec'\n",
    "    )\n",
    "    # inserindo estágios de transformação\n",
    "    stages += [indexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexação da variável resposta\n",
    "indexer = StringIndexer(\n",
    "    inputCol='y', \n",
    "    outputCol='label'\n",
    ")\n",
    "\n",
    "stages += [indexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformando variáveis numéricas para o tipo double\n",
    "for numericCol in numericColumns:\n",
    "    data = data.withColumn(numericCol, data[numericCol].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando assembler, que deixa os dados no formato vetorial \n",
    "# demandado pela biblioteca ML do Spark\n",
    "\n",
    "assembler_inputs = [categoricalCol+'_class_vec' for categoricalCol in categoricalColumns]\n",
    "assembler_inputs += numericColumns\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model = pipelineModel.transform(data)\n",
    "data_model = data_model.select([\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observações para treino: 28877\n",
      "Observações para teste:  12311\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = data_model.randomSplit([0.7, 0.3], seed=420)\n",
    "\n",
    "print('Observações para treino: {}'.format(trainingData.count()))\n",
    "print('Observações para teste:  {}'.format(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "|  y|               freq|\n",
      "+---+-------------------+\n",
      "| no| 0.8873458288821987|\n",
      "|yes|0.11265417111780131|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# verificando porcentagem de classes na variável resposta\n",
    "\n",
    "data.createOrReplaceTempView('data')\n",
    "label_count = spark.sql(\"\"\"\n",
    "SELECT \\\n",
    "    y, count(*) as freq \\\n",
    "FROM \\\n",
    "    data \\\n",
    "GROUP BY \\\n",
    "    y \\\n",
    "\"\"\")\n",
    "\n",
    "label_count.withColumn('freq', label_count.freq/data.count()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que o resultado positivo `yes` que buscamos prever é relativamente raro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "areaUnderROC = 0.7919\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f'areaUnderROC = {auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=30,\n",
    "    maxBins=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtModel = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = dtModel.transform(testData)\n",
    "predictions_dt_train = dtModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.8643\n",
      "Accuracy (TRAIN): 0.9689\n",
      "areaUnderROC:     0.6326\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "\n",
    "accuracy_dt = evaluator_accuracy.evaluate(predictions_dt)\n",
    "accuracy_dt_train = evaluator_accuracy.evaluate(predictions_dt_train)\n",
    "print(f'Accuracy:         {accuracy_dt:.4f}')\n",
    "print(f'Accuracy (TRAIN): {accuracy_dt_train:.4f}')\n",
    "auc_dt = evaluator_auc.evaluate(predictions_dt)\n",
    "print(f'areaUnderROC:     {auc_dt:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=20,\n",
    "    maxDepth=30,\n",
    "    seed=420\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = rf.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = rfModel.transform(testData)\n",
    "predictions_rf_train = rfModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.8952\n",
      "Accuracy (TRAIN): 0.9457\n",
      "areaUnderROC:     0.7820\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "\n",
    "accuracy_rf = evaluator_accuracy.evaluate(predictions_rf)\n",
    "accuracy_rf_train = evaluator_accuracy.evaluate(predictions_rf_train)\n",
    "print(f'Accuracy:         {accuracy_rf:.4f}')\n",
    "print(f'Accuracy (TRAIN): {accuracy_rf_train:.4f}')\n",
    "auc_rf = evaluator_auc.evaluate(predictions_rf)\n",
    "print(f'areaUnderROC:     {auc_rf:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    maxDepth=5,\n",
    "    seed=420\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel = gbt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt = gbtModel.transform(testData)\n",
    "predictions_gbt_train = gbtModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.9011\n",
      "Accuracy (TRAIN): 0.9065\n",
      "areaUnderROC:     0.8014\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "\n",
    "accuracy_gbt = evaluator_accuracy.evaluate(predictions_gbt)\n",
    "accuracy_gbt_train = evaluator_accuracy.evaluate(predictions_gbt_train)\n",
    "print(f'Accuracy:         {accuracy_gbt:.4f}')\n",
    "print(f'Accuracy (TRAIN): {accuracy_gbt_train:.4f}')\n",
    "auc_gbt = evaluator_auc.evaluate(predictions_gbt)\n",
    "print(f'areaUnderROC:     {auc_gbt:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.9011|   0.6598|0.2737|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_gbt.select('label', 'prediction').createOrReplaceTempView('predictions')\n",
    "\n",
    "aaa = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol = \"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel = svm.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_svm = svmModel.transform(testData)\n",
    "predictions_svm_train = svmModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.8863\n",
      "Accuracy (TRAIN): 0.8880\n",
      "areaUnderROC:     0.6682\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "\n",
    "accuracy_svm = evaluator_accuracy.evaluate(predictions_svm)\n",
    "accuracy_svm_train = evaluator_accuracy.evaluate(predictions_svm_train)\n",
    "print(f'Accuracy:         {accuracy_svm:.4f}')\n",
    "print(f'Accuracy (TRAIN): {accuracy_svm_train:.4f}')\n",
    "auc_svm = evaluator_auc.evaluate(predictions_svm)\n",
    "print(f'areaUnderROC:     {auc_svm:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.8863|   0.5096| 0.057|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_svm.select('label', 'prediction').createOrReplaceTempView('predictions')\n",
    "\n",
    "aaa = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = trainingData.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"]\n",
    "layers = [attrs, 100, 100, 100, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\", \n",
    "    layers=layers,\n",
    "    tol=1e-7,\n",
    "    seed = 420\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 ms, sys: 5.7 ms, total: 25 ms\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlpModel = mlp.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_mlp = mlpModel.transform(testData)\n",
    "predictions_mlp_train = mlpModel.transform(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:         0.8981\n",
      "Accuracy (TRAIN): 0.8973\n"
     ]
    }
   ],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_auc = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\"\n",
    ")\n",
    "\n",
    "\n",
    "accuracy_mlp = evaluator_accuracy.evaluate(predictions_mlp)\n",
    "accuracy_mlp_train = evaluator_accuracy.evaluate(predictions_mlp_train)\n",
    "print(f'Accuracy:         {accuracy_mlp:.4f}')\n",
    "print(f'Accuracy (TRAIN): {accuracy_mlp_train:.4f}')\n",
    "# auc_mlp = evaluator_auc.evaluate(predictions_mlp)\n",
    "# print(f'areaUnderROC:     {auc_mlp:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------+\n",
      "|accuracy|precision|recall|\n",
      "+--------+---------+------+\n",
      "|  0.8981|   0.6616|0.2174|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_mlp.select('label', 'prediction').createOrReplaceTempView('predictions')\n",
    "\n",
    "aaa = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    round((tp+tn)/(tp+tn+fp+fn), 4) as accuracy,\n",
    "    round(tp/(tp+fp), 4) as precision,\n",
    "    round(tp/(tp+fn), 4) as recall\n",
    "FROM (\n",
    "    SELECT\n",
    "        sum(tn) as tn,\n",
    "        sum(tp) as tp,\n",
    "        sum(fn) as fn,\n",
    "        sum(fp) as fp\n",
    "    FROM (\n",
    "        SELECT\n",
    "            case when label = 0 and prediction = 0 then 1 else 0 end as tn,\n",
    "            case when label = 1 and prediction = 1 then 1 else 0 end as tp,\n",
    "            case when label = 1 and prediction = 0 then 1 else 0 end as fn,\n",
    "            case when label = 0 and prediction = 1 then 1 else 0 end as fp\n",
    "        FROM\n",
    "            predictions\n",
    "    )\n",
    ")\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusão\n",
    "\n",
    "`GBTClassifier` apresentou maior potencial, apesar de ter um recall ainda muito baixo. Na sequência, vamos buscar encontrar melhores hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    seed=420\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 15, 20, 30]) \\\n",
    "    .addGrid(gbt.subsamplingRate, [.75, 1]) \\\n",
    "    .addGrid(gbt.maxIter, [60]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [20, 40, 60]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator = gbt,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 151 ms, sys: 44.8 ms, total: 196 ms\n",
      "Wall time: 3min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbtModel_cv = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7954570117058624, 0.7962791204553268, 0.7955932855441463]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtModel_cv.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtModel_cv.bestModel._java_obj.getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtModel_cv.bestModel._java_obj.getSubsamplingRate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
